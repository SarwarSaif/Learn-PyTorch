{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch Basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfBcYxHniqifCd55pwAdsS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarwarSaif/Learn-PyTorch/blob/main/PyTorch_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIl3FjzSX0WC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTaNB2Q6X6_O"
      },
      "source": [
        "# PyTorch Tensors\n",
        "Similar to Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B13CKG7wYA1n"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfyDIBgEYGyW"
      },
      "source": [
        "# Create empty tensors\n",
        "w = torch.empty(3) # empty 1D Tensor\n",
        "x = torch.empty(3, 2) # empty 2D Tensor\n",
        "y = torch.empty(3, 2, 4) # empty 3D Tensor\n",
        "z = torch.empty(3, 2, 4, 4) # empty 1D Tensor\n",
        "#print(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWNIwagkYN6s",
        "outputId": "276bf250-71c7-47d7-b98e-1fd50c6c157e"
      },
      "source": [
        "# Create tensors with random values\n",
        "x = torch.rand(2, 2)\n",
        "print(x)\n",
        "print(x.dtype)\n",
        "# Change type of the tensor\n",
        "x = torch.rand(2,2, dtype=torch.double)\n",
        "print(x)\n",
        "# Create tensors with arrays\n",
        "x = torch.tensor([2.5, 0.1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2950, 0.8424],\n",
            "        [0.1169, 0.5482]])\n",
            "torch.float32\n",
            "tensor([[0.5086, 0.1574],\n",
            "        [0.9693, 0.4536]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-4PVh78ZAKE",
        "outputId": "16571cf8-67a2-4313-c3f8-fc20537af5aa"
      },
      "source": [
        "x = torch.rand(2,2)\n",
        "y = torch.rand(2,2)\n",
        "z = x + y # Element wise add\n",
        "z = torch.add(x,y) # Same as Element wise\n",
        "print(z)\n",
        "\n",
        "# Inplace addition\n",
        "y.add_(x) # Modify only y\n",
        "# N.B: Every function with a trail sign '_' does an inplace operation\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.4582, 1.3041],\n",
            "        [0.3599, 0.2333]])\n",
            "tensor([[0.4582, 1.3041],\n",
            "        [0.3599, 0.2333]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D9Z1XiUaE8a",
        "outputId": "79d59b80-4d9e-4e70-fd47-42c8ffddbfe5"
      },
      "source": [
        "# Slicing operations on tensors\n",
        "x = torch.rand(5,3)\n",
        "print(x[1,2:4])\n",
        "# To get item or value from a single tensor\n",
        "print(x[1,1].item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.2339])\n",
            "0.1390628218650818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmJgO1wnbQ_u",
        "outputId": "c2d52eca-4809-4f3a-8ea5-2f35b67d3467"
      },
      "source": [
        "# Reshape the Tensors\n",
        "x = torch.rand(5, 4)\n",
        "print(x)\n",
        "y = x.view(20)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.9900, 0.8861, 0.4973, 0.3721],\n",
            "        [0.6714, 0.8887, 0.6617, 0.1334],\n",
            "        [0.9683, 0.7950, 0.6091, 0.9559],\n",
            "        [0.9372, 0.8076, 0.4903, 0.1536],\n",
            "        [0.2611, 0.6432, 0.6865, 0.3693]])\n",
            "tensor([0.9900, 0.8861, 0.4973, 0.3721, 0.6714, 0.8887, 0.6617, 0.1334, 0.9683,\n",
            "        0.7950, 0.6091, 0.9559, 0.9372, 0.8076, 0.4903, 0.1536, 0.2611, 0.6432,\n",
            "        0.6865, 0.3693])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Ruh50SfCTe",
        "outputId": "ee3a9f63-8ff0-47b1-abfb-50b953995518"
      },
      "source": [
        "\n",
        "# Cnvert tensor to a numpy array\n",
        "x = torch.ones(5) # <class 'torch.Tensor'>\n",
        "print(x, type(x)) \n",
        "y = x.numpy() # <class 'numpy.ndarray'>\n",
        "print(y, type(y))\n",
        "# N.B: If the tensor and numpy array is in running in the CPU than they will have same memory location. So changing x will change b and vice-versa"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1.]) <class 'torch.Tensor'>\n",
            "[1. 1. 1. 1. 1.] <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CllsnElGfGmi",
        "outputId": "c3b2ac90-5887-4513-b35b-f57b55cdc5cb"
      },
      "source": [
        "# Convert numpy array to tensor\n",
        "import numpy as np\n",
        "x = np.ones([2, 3])\n",
        "print(x)\n",
        "y = torch.from_numpy(x)\n",
        "print(y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSfdA3M6hNWD",
        "outputId": "ec65dad3-2bcf-4cdb-fa5a-614b01937146"
      },
      "source": [
        "# Use CUDA if available\n",
        "if torch.cuda.is_available():\n",
        "  print(\"Yes CUDA is available\")\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(device)\n",
        "  x = torch.ones(5, device=device) # Move x to the GPU [Way: 1]\n",
        "  y = torch.ones(5)\n",
        "  y = y.to(device) # Move y to the GPU [Way: 2]\n",
        "  z = x + y\n",
        "  print(z) \n",
        "  # N.B: now the GPU tensor can not be transformed into numpy using .numpy() method as numpy doesn't work on GPU and can only be used when using CPU\n",
        "  z = z.to(\"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Yes CUDA is available\n",
            "cuda\n",
            "tensor([2., 2., 2., 2., 2.], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow2fOmdbhMha",
        "outputId": "b213b1d6-3279-4de6-809c-11a719c2d99d"
      },
      "source": [
        "# When  you want to Optimize a varibale in future using gradient then you need to specify \"requires_grad=True\" which is by default False\n",
        "x = torch.ones(5, requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXmdEawukIUK"
      },
      "source": [
        "# Calculate Gradients using Autograd package in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga4G7ZZFj-uI",
        "outputId": "84ad7295-ca6a-4f95-a280-dcd2294039c9"
      },
      "source": [
        "import torch\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "y = x+2\n",
        "print(y)\n",
        "z = y.mean()\n",
        "z.backward()\n",
        "print(x.grad)\n",
        "# The Autograd function uses Jacobian Product to calculate the gradient\n",
        "# if the value is not scalar, we can use a vector to produce a scalar value and then can calculate the gradient\n",
        "v = torch.tensor([0.1, 1.0, 0.2], dtype=torch.float)\n",
        "y.backward(v)\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-1.0381,  0.0724, -0.5242], requires_grad=True)\n",
            "tensor([0.9619, 2.0724, 1.4758], grad_fn=<AddBackward0>)\n",
            "tensor([0.3333, 0.3333, 0.3333])\n",
            "tensor([0.4333, 1.3333, 0.5333])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ9A-9m6kb6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8361541-91e5-4979-f399-bd09923a43c8"
      },
      "source": [
        "# To stop pytorch from calculating Grad_fn function or tracking the history, wed can suse the following\n",
        "# x.requires_grad_(False)\n",
        "# x.detach()\n",
        "# with torch.no_grad():\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "with torch.no_grad():\n",
        "  y = x + 2\n",
        "  print(y)\n",
        "\n",
        "z = x + 3\n",
        "z.detach_()\n",
        "print(z)\n",
        "x.requires_grad_(False)\n",
        "print(x)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.8296,  0.1619,  1.2709], requires_grad=True)\n",
            "tensor([1.1704, 2.1619, 3.2709])\n",
            "tensor([2.1704, 3.1619, 4.2709])\n",
            "tensor([-0.8296,  0.1619,  1.2709])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR1ZcR07jXNK",
        "outputId": "00a3ce58-cfd3-4a71-f46b-96bf0cb7a4c4"
      },
      "source": [
        "# Dummy training example\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(4):\n",
        "  model_output = (weights*3).sum()\n",
        "\n",
        "  model_output.backward()\n",
        "\n",
        "  print(weights.grad)\n",
        "\n",
        "  # Before moving to next iteration we must empty our gradient\n",
        "  weights.grad.zero_()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p92MQUJmnnD0"
      },
      "source": [
        "# Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOfT-1FLlIzW",
        "outputId": "8bf20f7a-aa71-46bb-f073-3836b6adc603"
      },
      "source": [
        "import torch\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# froward pass and compute the loss\n",
        "y_hat = w * x\n",
        "loss = (y_hat - y) ** 2\n",
        "print(loss)\n",
        "\n",
        "# backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "# Update weights\n",
        "# Next forward and backward pass\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyC2Epff5Rs0"
      },
      "source": [
        "# Gradient Descent using AutoGrad and Backpropagation\n",
        "> Steps used in 1st Process:\n",
        "> * Prediction: Manually\n",
        "> * Gradients Computation: Manually\n",
        "> * Loss Computation: Manually\n",
        "> * Parameter updates: Manually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLvlbyVFmfGT",
        "outputId": "431f11f5-a214-44fb-e9b5-8c6944ca117d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# f = w * x # Neglect Bias Here\n",
        "# f = 2 * x # Goal\n",
        "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "# model prediction\n",
        "def forward(x: int) -> np.float32:\n",
        "  return w * x\n",
        "\n",
        "# loss = MSE (Mean Squarred Error)\n",
        "def loss(y:np.float32, y_predicted:np.float32) -> np.float32:\n",
        "  return ((y_predicted-y)**2).mean()\n",
        "\n",
        "# gradient \n",
        "# MSE = 1/N * (w*x - y)**2 \n",
        "# dJ/dw = 1/N * 2*x * (w*x -y) \n",
        "def gradient(x:int, y:np.float32, y_predicted:np.float32) -> np.float32:\n",
        "  return np.dot(2*x, (y_predicted-y)).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5): .3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 10\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients\n",
        "  dw = gradient(X,Y,y_pred)\n",
        "\n",
        "  # update weights\n",
        "  w -= learning_rate * dw\n",
        "  if epoch % 1 == 0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5): .3f}')\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction before training: f(5) =  0.000\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 2: w = 1.680, loss = 4.79999924\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 4: w = 1.949, loss = 0.12288000\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "epoch 6: w = 1.992, loss = 0.00314574\n",
            "epoch 7: w = 1.997, loss = 0.00050331\n",
            "epoch 8: w = 1.999, loss = 0.00008053\n",
            "epoch 9: w = 1.999, loss = 0.00001288\n",
            "epoch 10: w = 2.000, loss = 0.00000206\n",
            "Prediction after training: f(5) =  9.999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUAR14Xc9eQ_"
      },
      "source": [
        "> Steps used in 2nd Process:\n",
        "> * Prediction: Manually\n",
        "> * Gradients Computation: Autograd\n",
        "> * Loss Computation: Manually\n",
        "> * Parameter updates: Manually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnfY3cFU9Bhl",
        "outputId": "e2a3056d-ecba-4be7-dd39-a21ad49c70f6"
      },
      "source": [
        "import torch\n",
        "\n",
        "# f = w * x # Neglect Bias Here\n",
        "# f = 2 * x # Goal\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model prediction\n",
        "def forward(x: int) -> np.float32:\n",
        "  return w * x\n",
        "\n",
        "# loss = MSE (Mean Squarred Error)\n",
        "def loss(y:np.float32, y_predicted:np.float32) -> np.float32:\n",
        "  return ((y_predicted-y)**2).mean()\n",
        "\n",
        "# gradient is replaced by autograd \n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5): .3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients = backward_pass\n",
        "  l.backward() # Calculate the gradient dl/dw\n",
        "\n",
        "  # update weights\n",
        "  with torch.no_grad(): # Grad history tracking is not required here\n",
        "    dw = w.grad\n",
        "    w -= learning_rate * dw\n",
        "\n",
        "  # zero gradients // Or the Gradients will be summed in the next iteration\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if epoch % 2 == 0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5): .3f}')\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction before training: f(5) =  0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 3: w = 0.772, loss = 15.66018772\n",
            "epoch 5: w = 1.113, loss = 8.17471695\n",
            "epoch 7: w = 1.359, loss = 4.26725292\n",
            "epoch 9: w = 1.537, loss = 2.22753215\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 13: w = 1.758, loss = 0.60698116\n",
            "epoch 15: w = 1.825, loss = 0.31684780\n",
            "epoch 17: w = 1.874, loss = 0.16539653\n",
            "epoch 19: w = 1.909, loss = 0.08633806\n",
            "Prediction after training: f(5) =  9.612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkA-yNyz9kHb"
      },
      "source": [
        "> Steps used in 3rd Process:\n",
        "> * Prediction: Manually\n",
        "> * Gradients Computation: Autograd\n",
        "> * Loss Computation: PyTorch Loss\n",
        "> * Parameter updates: PyTorch Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwArcEDcpp9_",
        "outputId": "8495bf77-21c2-460c-e5a5-3c1bb3cb7e71"
      },
      "source": [
        "# 1 ) Design model (input, output size, forward pass)\n",
        "# 2 ) Construct loss and optimizer\n",
        "# 3 ) Training loop\n",
        "#     - forward pass: compute prediction\n",
        "#     - backward pass: gradients\n",
        "#     - update weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# f = w * x # Neglect Bias Here\n",
        "# f = 2 * x # Goal\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model prediction\n",
        "def forward(x: int) -> torch.float32:\n",
        "  return w * x\n",
        "\n",
        "\n",
        "# gradient is replaced by autograd \n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5): .3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients = backward_pass\n",
        "  l.backward() # Calculate the gradient dl/dw\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero gradients // Or the Gradients will be summed in the next iteration\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5): .3f}')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction before training: f(5) =  0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 21: w = 1.934, loss = 0.04506890\n",
            "epoch 31: w = 1.987, loss = 0.00174685\n",
            "epoch 41: w = 1.997, loss = 0.00006770\n",
            "epoch 51: w = 1.999, loss = 0.00000262\n",
            "epoch 61: w = 2.000, loss = 0.00000010\n",
            "epoch 71: w = 2.000, loss = 0.00000000\n",
            "epoch 81: w = 2.000, loss = 0.00000000\n",
            "epoch 91: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) =  10.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBOMFPga9qoL"
      },
      "source": [
        "> Steps used in 4th Process:\n",
        "> * Prediction: PyTorch Model\n",
        "> * Gradients Computation: Autograd\n",
        "> * Loss Computation: PyTorch Loss\n",
        "> * Parameter updates: PyTorch Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVueD4EWsCgk",
        "outputId": "c29edf7b-f7ea-45cc-a807-2f0b43512b50"
      },
      "source": [
        "# 1 ) Design model (input, output size, forward pass)\n",
        "# 2 ) Construct loss and optimizer\n",
        "# 3 ) Training loop\n",
        "#     - forward pass: compute prediction\n",
        "#     - backward pass: gradients\n",
        "#     - update weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# f = w * x # Neglect Bias Here\n",
        "# f = 2 * x # Goal\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "# model prediction\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "\n",
        "# gradient is replaced by autograd \n",
        "\n",
        "print(f'Prediction before training: f(5) = {model(X_test).item(): .3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 1000\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients = backward_pass\n",
        "  l.backward() # Calculate the gradient dl/dw\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero gradients // Or the Gradients will be summed in the next iteration\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "    #print(w)\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {model(X_test).item(): .3f}')\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 1\n",
            "Prediction before training: f(5) = -4.417\n",
            "Parameter containing:\n",
            "tensor([[-0.2634]], requires_grad=True)\n",
            "epoch 1: w = -0.263, loss = 67.48521423\n",
            "Parameter containing:\n",
            "tensor([[1.6186]], requires_grad=True)\n",
            "epoch 11: w = 1.619, loss = 1.74661422\n",
            "Parameter containing:\n",
            "tensor([[1.9218]], requires_grad=True)\n",
            "epoch 21: w = 1.922, loss = 0.04576396\n",
            "Parameter containing:\n",
            "tensor([[1.9711]], requires_grad=True)\n",
            "epoch 31: w = 1.971, loss = 0.00172560\n",
            "Parameter containing:\n",
            "tensor([[1.9795]], requires_grad=True)\n",
            "epoch 41: w = 1.979, loss = 0.00055470\n",
            "Parameter containing:\n",
            "tensor([[1.9813]], requires_grad=True)\n",
            "epoch 51: w = 1.981, loss = 0.00049472\n",
            "Parameter containing:\n",
            "tensor([[1.9821]], requires_grad=True)\n",
            "epoch 61: w = 1.982, loss = 0.00046521\n",
            "Parameter containing:\n",
            "tensor([[1.9826]], requires_grad=True)\n",
            "epoch 71: w = 1.983, loss = 0.00043811\n",
            "Parameter containing:\n",
            "tensor([[1.9831]], requires_grad=True)\n",
            "epoch 81: w = 1.983, loss = 0.00041261\n",
            "Parameter containing:\n",
            "tensor([[1.9836]], requires_grad=True)\n",
            "epoch 91: w = 1.984, loss = 0.00038860\n",
            "Parameter containing:\n",
            "tensor([[1.9841]], requires_grad=True)\n",
            "epoch 101: w = 1.984, loss = 0.00036598\n",
            "Parameter containing:\n",
            "tensor([[1.9846]], requires_grad=True)\n",
            "epoch 111: w = 1.985, loss = 0.00034468\n",
            "Parameter containing:\n",
            "tensor([[1.9850]], requires_grad=True)\n",
            "epoch 121: w = 1.985, loss = 0.00032462\n",
            "Parameter containing:\n",
            "tensor([[1.9855]], requires_grad=True)\n",
            "epoch 131: w = 1.985, loss = 0.00030572\n",
            "Parameter containing:\n",
            "tensor([[1.9859]], requires_grad=True)\n",
            "epoch 141: w = 1.986, loss = 0.00028793\n",
            "Parameter containing:\n",
            "tensor([[1.9863]], requires_grad=True)\n",
            "epoch 151: w = 1.986, loss = 0.00027117\n",
            "Parameter containing:\n",
            "tensor([[1.9867]], requires_grad=True)\n",
            "epoch 161: w = 1.987, loss = 0.00025539\n",
            "Parameter containing:\n",
            "tensor([[1.9871]], requires_grad=True)\n",
            "epoch 171: w = 1.987, loss = 0.00024052\n",
            "Parameter containing:\n",
            "tensor([[1.9875]], requires_grad=True)\n",
            "epoch 181: w = 1.988, loss = 0.00022652\n",
            "Parameter containing:\n",
            "tensor([[1.9879]], requires_grad=True)\n",
            "epoch 191: w = 1.988, loss = 0.00021334\n",
            "Parameter containing:\n",
            "tensor([[1.9882]], requires_grad=True)\n",
            "epoch 201: w = 1.988, loss = 0.00020092\n",
            "Parameter containing:\n",
            "tensor([[1.9886]], requires_grad=True)\n",
            "epoch 211: w = 1.989, loss = 0.00018923\n",
            "Parameter containing:\n",
            "tensor([[1.9889]], requires_grad=True)\n",
            "epoch 221: w = 1.989, loss = 0.00017821\n",
            "Parameter containing:\n",
            "tensor([[1.9893]], requires_grad=True)\n",
            "epoch 231: w = 1.989, loss = 0.00016784\n",
            "Parameter containing:\n",
            "tensor([[1.9896]], requires_grad=True)\n",
            "epoch 241: w = 1.990, loss = 0.00015807\n",
            "Parameter containing:\n",
            "tensor([[1.9899]], requires_grad=True)\n",
            "epoch 251: w = 1.990, loss = 0.00014887\n",
            "Parameter containing:\n",
            "tensor([[1.9902]], requires_grad=True)\n",
            "epoch 261: w = 1.990, loss = 0.00014020\n",
            "Parameter containing:\n",
            "tensor([[1.9905]], requires_grad=True)\n",
            "epoch 271: w = 1.990, loss = 0.00013204\n",
            "Parameter containing:\n",
            "tensor([[1.9907]], requires_grad=True)\n",
            "epoch 281: w = 1.991, loss = 0.00012436\n",
            "Parameter containing:\n",
            "tensor([[1.9910]], requires_grad=True)\n",
            "epoch 291: w = 1.991, loss = 0.00011712\n",
            "Parameter containing:\n",
            "tensor([[1.9913]], requires_grad=True)\n",
            "epoch 301: w = 1.991, loss = 0.00011030\n",
            "Parameter containing:\n",
            "tensor([[1.9915]], requires_grad=True)\n",
            "epoch 311: w = 1.992, loss = 0.00010388\n",
            "Parameter containing:\n",
            "tensor([[1.9918]], requires_grad=True)\n",
            "epoch 321: w = 1.992, loss = 0.00009783\n",
            "Parameter containing:\n",
            "tensor([[1.9920]], requires_grad=True)\n",
            "epoch 331: w = 1.992, loss = 0.00009214\n",
            "Parameter containing:\n",
            "tensor([[1.9923]], requires_grad=True)\n",
            "epoch 341: w = 1.992, loss = 0.00008678\n",
            "Parameter containing:\n",
            "tensor([[1.9925]], requires_grad=True)\n",
            "epoch 351: w = 1.992, loss = 0.00008173\n",
            "Parameter containing:\n",
            "tensor([[1.9927]], requires_grad=True)\n",
            "epoch 361: w = 1.993, loss = 0.00007697\n",
            "Parameter containing:\n",
            "tensor([[1.9929]], requires_grad=True)\n",
            "epoch 371: w = 1.993, loss = 0.00007249\n",
            "Parameter containing:\n",
            "tensor([[1.9931]], requires_grad=True)\n",
            "epoch 381: w = 1.993, loss = 0.00006827\n",
            "Parameter containing:\n",
            "tensor([[1.9933]], requires_grad=True)\n",
            "epoch 391: w = 1.993, loss = 0.00006430\n",
            "Parameter containing:\n",
            "tensor([[1.9935]], requires_grad=True)\n",
            "epoch 401: w = 1.994, loss = 0.00006055\n",
            "Parameter containing:\n",
            "tensor([[1.9937]], requires_grad=True)\n",
            "epoch 411: w = 1.994, loss = 0.00005703\n",
            "Parameter containing:\n",
            "tensor([[1.9939]], requires_grad=True)\n",
            "epoch 421: w = 1.994, loss = 0.00005371\n",
            "Parameter containing:\n",
            "tensor([[1.9941]], requires_grad=True)\n",
            "epoch 431: w = 1.994, loss = 0.00005058\n",
            "Parameter containing:\n",
            "tensor([[1.9943]], requires_grad=True)\n",
            "epoch 441: w = 1.994, loss = 0.00004764\n",
            "Parameter containing:\n",
            "tensor([[1.9944]], requires_grad=True)\n",
            "epoch 451: w = 1.994, loss = 0.00004487\n",
            "Parameter containing:\n",
            "tensor([[1.9946]], requires_grad=True)\n",
            "epoch 461: w = 1.995, loss = 0.00004225\n",
            "Parameter containing:\n",
            "tensor([[1.9948]], requires_grad=True)\n",
            "epoch 471: w = 1.995, loss = 0.00003980\n",
            "Parameter containing:\n",
            "tensor([[1.9949]], requires_grad=True)\n",
            "epoch 481: w = 1.995, loss = 0.00003748\n",
            "Parameter containing:\n",
            "tensor([[1.9951]], requires_grad=True)\n",
            "epoch 491: w = 1.995, loss = 0.00003530\n",
            "Parameter containing:\n",
            "tensor([[1.9952]], requires_grad=True)\n",
            "epoch 501: w = 1.995, loss = 0.00003324\n",
            "Parameter containing:\n",
            "tensor([[1.9954]], requires_grad=True)\n",
            "epoch 511: w = 1.995, loss = 0.00003131\n",
            "Parameter containing:\n",
            "tensor([[1.9955]], requires_grad=True)\n",
            "epoch 521: w = 1.995, loss = 0.00002949\n",
            "Parameter containing:\n",
            "tensor([[1.9956]], requires_grad=True)\n",
            "epoch 531: w = 1.996, loss = 0.00002777\n",
            "Parameter containing:\n",
            "tensor([[1.9958]], requires_grad=True)\n",
            "epoch 541: w = 1.996, loss = 0.00002615\n",
            "Parameter containing:\n",
            "tensor([[1.9959]], requires_grad=True)\n",
            "epoch 551: w = 1.996, loss = 0.00002463\n",
            "Parameter containing:\n",
            "tensor([[1.9960]], requires_grad=True)\n",
            "epoch 561: w = 1.996, loss = 0.00002320\n",
            "Parameter containing:\n",
            "tensor([[1.9961]], requires_grad=True)\n",
            "epoch 571: w = 1.996, loss = 0.00002185\n",
            "Parameter containing:\n",
            "tensor([[1.9962]], requires_grad=True)\n",
            "epoch 581: w = 1.996, loss = 0.00002058\n",
            "Parameter containing:\n",
            "tensor([[1.9963]], requires_grad=True)\n",
            "epoch 591: w = 1.996, loss = 0.00001938\n",
            "Parameter containing:\n",
            "tensor([[1.9965]], requires_grad=True)\n",
            "epoch 601: w = 1.996, loss = 0.00001825\n",
            "Parameter containing:\n",
            "tensor([[1.9966]], requires_grad=True)\n",
            "epoch 611: w = 1.997, loss = 0.00001719\n",
            "Parameter containing:\n",
            "tensor([[1.9967]], requires_grad=True)\n",
            "epoch 621: w = 1.997, loss = 0.00001619\n",
            "Parameter containing:\n",
            "tensor([[1.9968]], requires_grad=True)\n",
            "epoch 631: w = 1.997, loss = 0.00001525\n",
            "Parameter containing:\n",
            "tensor([[1.9969]], requires_grad=True)\n",
            "epoch 641: w = 1.997, loss = 0.00001436\n",
            "Parameter containing:\n",
            "tensor([[1.9969]], requires_grad=True)\n",
            "epoch 651: w = 1.997, loss = 0.00001352\n",
            "Parameter containing:\n",
            "tensor([[1.9970]], requires_grad=True)\n",
            "epoch 661: w = 1.997, loss = 0.00001274\n",
            "Parameter containing:\n",
            "tensor([[1.9971]], requires_grad=True)\n",
            "epoch 671: w = 1.997, loss = 0.00001199\n",
            "Parameter containing:\n",
            "tensor([[1.9972]], requires_grad=True)\n",
            "epoch 681: w = 1.997, loss = 0.00001130\n",
            "Parameter containing:\n",
            "tensor([[1.9973]], requires_grad=True)\n",
            "epoch 691: w = 1.997, loss = 0.00001064\n",
            "Parameter containing:\n",
            "tensor([[1.9974]], requires_grad=True)\n",
            "epoch 701: w = 1.997, loss = 0.00001002\n",
            "Parameter containing:\n",
            "tensor([[1.9975]], requires_grad=True)\n",
            "epoch 711: w = 1.997, loss = 0.00000944\n",
            "Parameter containing:\n",
            "tensor([[1.9975]], requires_grad=True)\n",
            "epoch 721: w = 1.998, loss = 0.00000889\n",
            "Parameter containing:\n",
            "tensor([[1.9976]], requires_grad=True)\n",
            "epoch 731: w = 1.998, loss = 0.00000837\n",
            "Parameter containing:\n",
            "tensor([[1.9977]], requires_grad=True)\n",
            "epoch 741: w = 1.998, loss = 0.00000788\n",
            "Parameter containing:\n",
            "tensor([[1.9977]], requires_grad=True)\n",
            "epoch 751: w = 1.998, loss = 0.00000742\n",
            "Parameter containing:\n",
            "tensor([[1.9978]], requires_grad=True)\n",
            "epoch 761: w = 1.998, loss = 0.00000699\n",
            "Parameter containing:\n",
            "tensor([[1.9979]], requires_grad=True)\n",
            "epoch 771: w = 1.998, loss = 0.00000659\n",
            "Parameter containing:\n",
            "tensor([[1.9979]], requires_grad=True)\n",
            "epoch 781: w = 1.998, loss = 0.00000620\n",
            "Parameter containing:\n",
            "tensor([[1.9980]], requires_grad=True)\n",
            "epoch 791: w = 1.998, loss = 0.00000584\n",
            "Parameter containing:\n",
            "tensor([[1.9981]], requires_grad=True)\n",
            "epoch 801: w = 1.998, loss = 0.00000550\n",
            "Parameter containing:\n",
            "tensor([[1.9981]], requires_grad=True)\n",
            "epoch 811: w = 1.998, loss = 0.00000518\n",
            "Parameter containing:\n",
            "tensor([[1.9982]], requires_grad=True)\n",
            "epoch 821: w = 1.998, loss = 0.00000488\n",
            "Parameter containing:\n",
            "tensor([[1.9982]], requires_grad=True)\n",
            "epoch 831: w = 1.998, loss = 0.00000460\n",
            "Parameter containing:\n",
            "tensor([[1.9983]], requires_grad=True)\n",
            "epoch 841: w = 1.998, loss = 0.00000433\n",
            "Parameter containing:\n",
            "tensor([[1.9983]], requires_grad=True)\n",
            "epoch 851: w = 1.998, loss = 0.00000408\n",
            "Parameter containing:\n",
            "tensor([[1.9984]], requires_grad=True)\n",
            "epoch 861: w = 1.998, loss = 0.00000384\n",
            "Parameter containing:\n",
            "tensor([[1.9984]], requires_grad=True)\n",
            "epoch 871: w = 1.998, loss = 0.00000362\n",
            "Parameter containing:\n",
            "tensor([[1.9985]], requires_grad=True)\n",
            "epoch 881: w = 1.998, loss = 0.00000341\n",
            "Parameter containing:\n",
            "tensor([[1.9985]], requires_grad=True)\n",
            "epoch 891: w = 1.999, loss = 0.00000321\n",
            "Parameter containing:\n",
            "tensor([[1.9986]], requires_grad=True)\n",
            "epoch 901: w = 1.999, loss = 0.00000302\n",
            "Parameter containing:\n",
            "tensor([[1.9986]], requires_grad=True)\n",
            "epoch 911: w = 1.999, loss = 0.00000284\n",
            "Parameter containing:\n",
            "tensor([[1.9986]], requires_grad=True)\n",
            "epoch 921: w = 1.999, loss = 0.00000268\n",
            "Parameter containing:\n",
            "tensor([[1.9987]], requires_grad=True)\n",
            "epoch 931: w = 1.999, loss = 0.00000252\n",
            "Parameter containing:\n",
            "tensor([[1.9987]], requires_grad=True)\n",
            "epoch 941: w = 1.999, loss = 0.00000238\n",
            "Parameter containing:\n",
            "tensor([[1.9988]], requires_grad=True)\n",
            "epoch 951: w = 1.999, loss = 0.00000224\n",
            "Parameter containing:\n",
            "tensor([[1.9988]], requires_grad=True)\n",
            "epoch 961: w = 1.999, loss = 0.00000211\n",
            "Parameter containing:\n",
            "tensor([[1.9988]], requires_grad=True)\n",
            "epoch 971: w = 1.999, loss = 0.00000199\n",
            "Parameter containing:\n",
            "tensor([[1.9989]], requires_grad=True)\n",
            "epoch 981: w = 1.999, loss = 0.00000187\n",
            "Parameter containing:\n",
            "tensor([[1.9989]], requires_grad=True)\n",
            "epoch 991: w = 1.999, loss = 0.00000176\n",
            "Prediction after training: f(5) =  9.998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQzLyXFXvPS8"
      },
      "source": [
        "## Custom Linear Regression Model in Step 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACKj-6uUsDFJ",
        "outputId": "c8cb6aaf-8724-4a4b-efbb-04b995ce79d8"
      },
      "source": [
        "# 1 ) Design model (input, output size, forward pass)\n",
        "# 2 ) Construct loss and optimizer\n",
        "# 3 ) Training loop\n",
        "#     - forward pass: compute prediction\n",
        "#     - backward pass: gradients\n",
        "#     - update weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# f = w * x # Neglect Bias Here\n",
        "# f = 2 * x # Goal\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "# model prediction\n",
        "# Turn this \"model = nn.Linear(input_size, output_size)\"  into a custom class \n",
        "class LinearRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LinearRegression, self).__init__()\n",
        "    # define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "print(f'Prediction before training: f(5) = {model(X_test).item(): .3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.1\n",
        "n_iters = 400\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients = backward_pass\n",
        "  l.backward() # Calculate the gradient dl/dw\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero gradients // Or the Gradients will be summed in the next iteration\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 50 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "    #print(w)\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {model(X_test).item(): .3f}')\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 1\n",
            "Prediction before training: f(5) = -2.031\n",
            "epoch 1: w = 3.005, loss = 41.11968231\n",
            "epoch 51: w = 1.913, loss = 0.01163712\n",
            "epoch 101: w = 1.981, loss = 0.00055687\n",
            "epoch 151: w = 1.996, loss = 0.00002665\n",
            "epoch 201: w = 1.999, loss = 0.00000127\n",
            "epoch 251: w = 2.000, loss = 0.00000006\n",
            "epoch 301: w = 2.000, loss = 0.00000000\n",
            "epoch 351: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) =  10.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXSkiQCXzke8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}